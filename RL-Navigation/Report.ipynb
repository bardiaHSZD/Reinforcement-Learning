{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Navigation\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this project, an agent is trained to navigate (and collect bananas!) in a large, square world. In this interction with the environment, the agent takes acetions and observes the feedback from the world in terms of rewards. The goal is to achieve higher rewards and less error in chossing bananas (i.e. choosing the yellow bananas instead of blue bananas). For learning from rewards and penalties of its actions, agent uses 2 separate deep neural networks with exact same architectures, local and target networks, as well as a replay memory for using the experiences to improve its performance. The replay memory keeps a buffer of states, actions, next_states and rewards for the target network.\n",
    "\n",
    "Accordingly, the target network is initialized randomly at first and gets updated after memory is large enough. It picks batches of random samples from memory and tunes the weights and biases based on the experiences to acheive best action for a given state. On the other hand, the local network takes a state as input and outputs an action to minimize the error from Target network. The ultimate goal is to achieve an average score of +13 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"banana-intro.gif\" \n",
    "\ttitle=\"Trained Agent\" width=\"450\" height=\"300\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters:\n",
    "\n",
    "n_episodes (int) = 2000 (i.e. maximum number of training episodes)\n",
    "\n",
    "max_t (int) = 1000 maximum number of timesteps per episode)\n",
    "\n",
    "eps_start (float) = 1.0 (i.e. starting value of epsilon, for epsilon-greedy action selection)\n",
    "\n",
    "eps_end (float) = 0.1 (i.e. minimum value of epsilon)\n",
    "\n",
    "eps_decay (float) = 0.995 (i.e. multiplicative factor (per episode) for decreasing epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture:\n",
    "\n",
    "The DQN agent is initialized with two FeedForward Networks (i.e. local and target) and a replay buffer with starting epsilon 1.0. The agent observes the current state and calls the act module. It can randomly take an action or pass the state throught the local network. As epsilon close to 1.0 means more exploration in the environment, and as epsilon gets smaller (down to 0.1) the agent has less chances to explore that gained through the local network and more exploitation. Actions are passed to the environment and reward and the next state of the environment are returned. Then agent sends the observations to the step() module where state, action, reward, next_state and done are added to replay buffer for the target network. As the memory size has reached a threshold, agent collects a batch of random sample for learning for minimizing local error and updates the target network. Ultimately, the agent completes the episodes until it is maxed out. \n",
    "\n",
    "The Q-Nework consists of 3 feed forward layers:\n",
    "\n",
    "- fc1 : in:state_size, out:64\n",
    "\n",
    "- relu: activation for adding nonlinearity\n",
    "\n",
    "- fc2: in:64, out:64\n",
    "\n",
    "- relu: activation for adding nonlinearity\n",
    "\n",
    "- fc3: in:64, out:action_size\n",
    "\n",
    "\n",
    "\n",
    "Here is the result of training the agent and recording the average core per 100 episodes:\n",
    "\n",
    "Action Size:4, State Size:37\n",
    "\n",
    "- Episode 100\tAverage Score: 1.26\n",
    "\n",
    "- Episode 200\tAverage Score: 4.76\n",
    "\n",
    "- Episode 300\tAverage Score: 7.79\n",
    "\n",
    "- Episode 400\tAverage Score: 10.93\n",
    "\n",
    "- Episode 500\tAverage Score: 13.22\n",
    "\n",
    "- Episode 600\tAverage Score: 14.47\n",
    "\n",
    "- Episode 700\tAverage Score: 15.29\n",
    "\n",
    "- Episode 800\tAverage Score: 14.99\n",
    "\n",
    "- Episode 900\tAverage Score: 16.40\n",
    "\n",
    "- Episode 1000\tAverage Score: 15.98\n",
    "\n",
    "- Episode 1100\tAverage Score: 16.54\n",
    "\n",
    "- Episode 1200\tAverage Score: 15.65\n",
    "\n",
    "- Episode 1300\tAverage Score: 15.53\n",
    "\n",
    "- Episode 1400\tAverage Score: 15.55\n",
    "\n",
    "- Episode 1500\tAverage Score: 16.01\n",
    "\n",
    "- Episode 1600\tAverage Score: 15.58\n",
    "\n",
    "- Episode 1700\tAverage Score: 15.28\n",
    "\n",
    "- Episode 1800\tAverage Score: 15.31\n",
    "\n",
    "- Episode 1900\tAverage Score: 15.66\n",
    "\n",
    "- Episode 2000\tAverage Score: 15.56\n",
    "\n",
    "Note: Score of +13 was achieved within 500 episodes; however, I continued training until 2000 episodes to show an stable solution was obtained.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"plot.png\" \n",
    "\ttitle=\"Rewards Per Episode\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Codes:\n",
    "\n",
    "model.py\n",
    "\n",
    "dqn_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas For Future Work\n",
    "- Applying Double DQN for the overestimation of Q-values\n",
    "- Implementing the deuling DQN\n",
    "- Prioritized experience replay\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
